<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
/**
 * @file Coders Import HTML Module for Drupal
 * @package coders
 * @author Dan Morrison http://coders.co.nz/
 * @version $Id: import_html_help.htm,v 1.9.2.1 2007/02/19 13:50:26 dman Exp $
 */
-->

<style type="text/css">
/*<![CDATA[*/
      .todo {color:00AA00;}
      .beta {color:#007700;}
/*]]>*/
</style>

<title>Drupal Module: Import_HTML</title>
</head>

<body id="content">
<h1>Drupal Module: Import_HTML</h1>

<h2>Synopsis</h2>

<p>Facility to import an existing, static HTML site structure into
Drupal Nodes.</p>

<p>This is done by allowing an admin to define a source directory of a
traditional HTML website, and importing (as much as possible) the
content and structure into a drupal site.</p>

<p>Files will be absorbed completely, and their existing cross-links
should be maintained, whilst the standard headers, chrome and navigation
blocks should be stripped and replaced with Drupal equvalents. Old
structure will be inferred and imported from the old folder heirachy.</p>

<ul>
	<li><a href='#Requirements'>Requirements</a></li>

	<li><a href='#Usage'>Usage</a> Detailed step-by-step</li>

	<li><a href='#Intent'>Intent</a> What it's indended to to

	<ul>
		<li><a href='#Methodology'>Methodology</a> Exactly how it does it, at
		a coder level</li>

		<li><a href='#Notes'>Notes</a> Issues arising, and some detailed
		explanations</li>
	</ul>
	</li>

	<li><a href='#Guide'>Guide</a> Reference section

	<ul>
		<li><a href='#Setup'>Setup</a> Requirements, and Installing for the
		first time</li>

		<li><a href='#Templates'>Import Templates</a> XSL. With great power
		comes great complexity</li>

		<li><a href='#Settings'>Settings</a> Explanation of the user settings</li>
	</ul>
	</li>

	<li><a href='#Development'>Development / TODO</a></li>
</ul>

<h2><a name='Requirements' id="Requirements">Requirements</a></h2>

<h3>Before you begin</h3>

<p>See the <a href="#setup">setup section</a> for details. Because of
the number of settings, this is not just a point-and-go module.</p>

<ul>
	<li>XML/XSLT support on the server. Check your php_info(). (XML under
	PHP 4.3 has some limitations, <a href="#trouble">see below</a>.)</li>

	<li>HTMLTidy - Either with the PHP module or the commandline version.</li>

	<li>Some understanding of XSL for advanced template translation.</li>

	<li>Some libraries of my own (bundled) to actually do the XSLT</li>
</ul>

<h2><a name='Usage' id="Usage">Usage</a></h2>

<p>This module uses no database tables of its own. It requires XML
support on the server, this can be tricky if it's not already enabled.</p>

<p>Given a working system, the process is thus:</p>

<ol>
	<li>Visit the <a href='admin/settings/import_html'>admin/settings/import_html</a>
	page and <a href='#Options'>check the settings</a>.</li>

	<li>If all values look OK for now, you can try a test run by visiting
	admin/import_html/demo . Choose a 'page' sort of page, not a portal or
	layout-rich sort of thing. The demo will scrape the given file and
	import it to the system. Some of the new navigation features will not
	be apparent yet, as they apply only to large-scale imports, or at least
	imports that have a defined siteroot.</li>

	<li>Try opening the '<a href='admin/settings/import_html'>admin/import_html</a>'
	main page and defining a source folder. Enter the root path of the site
	you wish to import and continue. <a href="#Treeview">The UI should
	display a treeview</a> of the files you can selectively select for
	import. <br />
	It's recommended to just try one page at a time to begin with. <br />
	<strong>Note:</strong> If your server has <a
		href="http://nz2.php.net/manual/en/features.safe-mode.php#ini.open-basedir">PHP
	open_basedir</a> restrictions in effect, the webserver/PHP process may
	be prevented from accessing files outside of webroot. <a
		href="#open_basedir">See below</a></li>

	<li>Upon importing a page, a new node should be created. The object of
	the import templates is to trim down the <i>content</i> block to its
	unique value. This will <i>probably</i> require some template tuning,
	so make a new template (copy the existing html2drupal.xsl), select it
	(enter the new name in the admin page) <a
		href="http://www.dpawson.co.uk/xsl/sect2/sect21.html">tweak the XSL</a>
	and try again. <br />
	If you are extremely lucky, or don't care too much about the extras,
	you can go straight to bulk import.</li>

	<li>If you need to check how the the images are turning up, they can
	safely be imported as well using the previous interface. They will be
	copied, structured in the same folders they were in originally, into
	the directory configured in the admin/setting. Imported pages will have
	their links rewritten to find them there. <br />
	Two type of content are being imported, depending on file suffix.
	'Pages' (html) - which become nodes ... and everything else, which
	becomes 'files'. <span class='beta'>File suffixes are not good enough
	for this, should the suffix list be editable, or should I scan the
	files themselves?</span></li>

	<li>When you are happy that the body field is as tidy as it's going to
	get (test several pages), you can try a bulk import. This may fill up
	your node collection a bit, so be prepared to delete them if things
	don't work perfectly first time. Many static sites have whole sections
	that are not structured the same as the rest of the pages.</li>

	<li>On input, a menu structure and a bunch of aliases will be
	auto-generated. These can be manually adjusted easily. For instance,
	the menu branches will initially be named after the document titles
	found in the directory structure. Which is great if you used a decent
	folder heirachy, but some of the labels can probably be tidied up a
	bit. For that matter, after input, you can safely re-arrange the menu
	structure altogether, shifting whole sections to different places
	without worrying about links breaking. These changes will show through
	in the menu, sitemap and breadcrumbs <em>but not in the pathalias</em>
	which will reman old-style. <span class='beta'>There appear to be
	issues navigating to pages deep in a menu where the parent has not been
	imported or created yet. This is normal Drupal behaviour when making
	menu links to non-existant paths.</span></li>
</ol>

<p>By following these instructions, you should probably be able to end
up with a version of the old content in the new layout. For large sites
(200+ pages) some extra tuning may be neccessary, eg using different
templates for different sources.</p>

<p>Incremental imports, processing just sections at a time, or repeated
imports as you tune the content or the transformation should be
non-destructive. Re-importing the same file will retain the same node ID
path, and any Drupal-specific additions made so far.</p>

<h2><a name='Intent' id="Intent">Intent / Theory</a></h2>

<p>This is intended as a run-once sort of tool, that, once tuned right
on a handful of pages, can churn through a large number of reasonably
structured, reasonably formatted pages doing a lot of the boring copy
&amp; paste that would otherwise be required.</p>

<p>The existing file paths of the source content will be used to create
an automatic menu, and therefore a heirachical structure identical to
the source URLs. With path.module, appropriate aliases will also be
created such that this will enable a drupal instance to TRANSPARENTLY
REPLACE an existing static site without breaking any bookmarks!</p>

<h3><a name='Methodology' id='Methodology'>Methodology Overview / Tasks</a></h3>

<p>A peek under the hood into what happens in what order</p>

<ul>
	<li>Facility for spidering/enumerating existing source files. (the
	admin/import_html page)</li>

	<li>Define import rules - choose an XSL stylesheet, set some parameters
	on it, <span class='todo'>configure presets for the imported pages</span>.</li>

	<li>Expose selective selection of files to import (admin UI) <br />
	<img src='import_html_select_files.png' alt='[screenshot]' /></li>

	<li>Import each source file by way of sequential

	<ol>
		<li>(Optional) <span class='beta'>download/copy of files to local
		mirror site.</span></li>

		<li>Processing with html-tidy, to prepare for XSL transforms</li>

		<li>URL-rewriting via XSL. All hrefs are redirected to the new
		pseudo-location aliases, all srcs are redirected to somewhere under <kbd>/files</kbd>.</li>

		<li>Content-scraping via XSL (XSL stylesheet will probably have to be
		customized to each source site)</li>

		<li>Or content-scraping via RegExps and heuristics</li>

		<li>Deduction (as much as possible) of meta-information like page
		title, author,date</li>

		<li>Validate nodes and save them with node-insert calls</li>

		<li>Extra API-insert calls to create menu navigation and path aliases.
		Taxonomy leverage and some extras were also added thanks to L0rne Oct
		2006. Two types of alias: <kbd>/path/filename</kbd> and <kbd>/path/filename<b>.html</b></kbd>
		can be created for now.</li>
	</ol>
	</li>

	<li>Pages are now first-class nodes, and can be administered through
	the CMS as usual.</li>
</ul>

<h3><a name='Notes' id="Notes">Notes</a></h3>

<p>The more valid and more homogenous the source site is, the better. A
creation using strict XHTML and useful, semantic tags like <kbd>#title</kbd>
<kbd>#content</kbd> or something could be imported swiftly. One with a
variety of table structures may not... <br />
Of course, this tool is supposed to be useful when dealing with messy,
non-homogenous legacy sites that need a makeover. <span class='todo'>Sometimes
regular expression parsing may come to the rescue for content
extraction, but that's not implimented yet.</span></p>

<p>I'm choosing XSL because I know it, it's powerful for converting
content out of (well-structured) HTML, and I've had success with this
approach in the past. Others may object to this abstract technology (XSL
is NOT an easy learning curve) but the alternative options include
RegExp wierdness or cut and paste. (<span class='todo'>which I may patch
on as alternative methods - or someone else can have a go</span>) Both
approaches I've also used successfully in bulk site templating (over
THOUSANDS of pages) but it's my call. <a href='#Templates'>Making your
own XSL import template</a> is non-trivial.</p>

<p>In the interests of good housekeeping, <b>imported files with spaces in the 
filenames will be renamed to use underscores</b>. 
Although it spaces <em>can</em>em> be worked around, they just cause trouble in
website URLs. Thus, references to the spaced, or %20 versions of the files
may break. This rewrite can be disabled in the settings.
<br/>Filenames are assumed to be, and will remain, case-sensitive.
</p>

<h2><a name='Guide' id="Guide">Guide</a></h2>

<h3><a name='setup' id="setup">Installation/setup</a></h3>

<h4>XML Support</h4>

<p>The module can use either the PHP4 and PHP5 implimentations (which
are quite different) but the PHP modules <b>do</b> have to be enabled
somehow. This can be tricky as they often require extra libraries to be
put in your path somewhere. Please don't ask me for instructions, every
time I've done it it hurts my head.</p>
<p>If you can see the words XSL or XSLT in your phpinfo() output, You
should be fine. The module will test and warn you anyway.</p>
<p></p>
<p>PHP 4.3 has at least <a href="#trouble">one known bug</a>.</p>
<h4>HTMLTidy Setup</h4>

<p>The module also uses <a href="http://www.w3.org/People/Raggett/tidy/">the
famous HTMLTidy tool</a>. There is now <a
	href="http://www.zend.com/php5/articles/php5-tidy.php">a PHP module
that impliments HTMLTidy natively</a>, but that needs to be installed
and enabled. If you don't have access to that, we can run it from the
command line. Find the <a
	href="http://tidy.sourceforge.net/docs/Overview.html#download">appropriate
binary release of HTMLTidy</a> for your system, and place it in your
PATH, in the modules install directory, or wherever you like, then <em>define
the path to the executable <a href='admin/settings/import_html'>in the
settings</a></em>. This works fine under Windows too.</p>
<p>If this sounds complicated, and you have limited access to a Unix
host and need to use it, there is an auto-installer that can attempt to
set up tidy even on a box you don't have login access to.</p>

<h3><a name='Templates' id="Templates">Import Templates</a></h3>

<p>An import template defines the mapping between existing HTML content
and our node values. It uses the XSL language because of the power it
has to select bits of a structured document, for example <code>select=\"//*[@id='content']\"</code>
... will find the block anywhere in the page, of any type with the id
'content', and <code>select=\"//table[@class='main']//td[position(3)]\"</code>
Will locate the third TD block in the table called 'main'. Both these
examples would be common when trying to extract the actual <em>text</em>
from a legacy site.</p>

<p>You can begin with the example XSL template, this contains code that
attempts to translate a page containing the usual HTML structures like
(either title or h1) and (either the div called 'content' or the entire
body tag) into a standard, minimal, vanilla, sematically-tagged HTML
doc.</p>

<p>It's likely that whatever site you are importing will NOT be shaped
exactly like we need it to translate straight using this format. You
have to identify the parts of your existing pages that can reliably be
scanned for to define content, then come up with an <a
	href='http://www.w3.org/TR/xpath'>XPath expression</a> to represent
this.</p>

<p>If your source, for example, didn't use nice H1 tage to denote the
page title, but instead always looked like</p>
<pre>
&lt;font size='+2'&gt;&lt;B&gt;my
  page&lt;/B&gt;&lt;/font&gt;
</pre>
<p>... your template could be made to find it, wherever it was in the
page using <code>select=\"//font[@size='+2']/B\"</code> and proceed to
use that as the node title.</p>

<p>No, the code is not pretty, and if Regular Expressions are a foreign
language to you, This is worse. <br />
But <em>this</em> is why developers have been ranting for the last ten
years about using semantic markup!! <br />
The uniformity, and the usefulness of the metadata detected in the
source files will play a big part here.</p>

<p>It's easier to develop and test the XSLT using a third-party tool, I
recommend <a href="http://www.xmlcooktop.com/">Cooktop</a>. Be sure to
set the XSL engine to 'Sablotron' which is the one that PHP uses under
the hood.</p>

<p><span class='todo'>Although it would be <em>possible</em> to
configure a logical mapping system to select different import templates
based on different content, at this stage the administrator is expected
to be doing a bit of hand-tweaking, and predicting all possible inputs
is impossible.</span> <span class='beta'>Some of this sort of logic <em>can</em>
however be built into the powerful XSL template, if you are good at XSL</span></p>

<p>Once importing is taking place, you can even filter it more to
improve the structure of the input, for example by removing all
redundant FONT tags, or by ensuring that every H1,2,3 tag has an
associated #ID for anchoring. Yay XSL.</p>

<h4 id='import_to_cck'>Import to CCK</h4>
<p>The base functionality supports placing found content into the <code>$node->body</code>
field, not naturally into any arbitrary CCK fields, but this is also
possible.</p>
<p>If you have a CCK node with (eg) fields:</p>
<pre>field_text, field_byline, field_image</pre>
and your input pages are nice and semantically tagged, eg
<pre>
&lt;body&gt;
  &lt;h1 id='title'&gt;the title&lt;/h1&gt;
  &lt;div id='image'&gt;&lt;img src='this.gif'/&gt;&lt;/div&gt;
  &lt;h3 id='byline'&gt;By me&lt;/h3&gt;
  &lt;div id='text'&gt;the content html etc&lt;/div&gt;
&lt;/body&gt;
</pre>
<p>A mapping from HTML ids to CCK fields will be done automatically, and
the content <em>should</em> just fall into place.</p>
<pre>
  $node-&gt;title = "the title";
  $node-&gt;field_image = "&lt;img src='this.gif'/&gt;&lt;";
  $node-&gt;field_byline = "By me";
  $node-&gt;field_text = "the content html etc";
</pre>

<p>In fact, ANY element found in the source text with an ID or class
gets added to the <code>$node</code> object during import, although most
data found this way is immediatly discarded again if the content type
doesn't know how to serialize it. <br />
A special-case demonstrated here prepends <code>field_</code> to known
CCK field names. Normally they get labelled as-is.</p>
<p>If the source data is NOT tagged, you'll have to develop a bit of
custom XSL to produce the same effect.</p>
<h4>customtemplate2simplehtml.xsl</h4>
<pre>
... xsl preamble ...
  &lt;xsl:template name="html_doc" match="/"&gt;	
    &lt;html&gt;
    &lt;body&gt;
    ... other extractions ...
    &lt;h3 id="byline"&gt;
      &lt;xsl:value-of select="./descendant::xhtml:img[2]/@alt" /&gt;
    &lt;/h3&gt;
    &lt;/body&gt;
  &lt;/html&gt;
&lt;/xsl:template>
</pre>
<p>In this example, the byline we wanted to extract was the alt value of
the second image found in the page (a real-world example). This has now
been extracted and wrapped in an ID-ed h3 during an early phase of the
import process, and should now turn up in the CCK field_byline as
desired. <br />
XSL is complex, but magic.</p>

<h3><a name='Settings' id="Settings">Settings</a></h3>

<p>On the admin/settings/import_html screen, you can (if you wish):</p>

<ul>
	<li>Choose the <a href='#Templates'>import template</a>. These
	templates translate between the existing page structure and the raw
	content blocks. <span class='beta'>Currently you enter the name of the
	xsl file directly. If I make this a select, some flexibility is lost</span></li>

	<li>Customize a parameter used in the import template - the id of the
	real 'content' block of the source documents. <span class='todo'>This
	could be extended into a wizard to work towards an all-purpose
	template, but that will probably never happen. Can't predict how broken
	the import sites may be.</span></li>

	<li>When a site is imported, it must bring along some of its baggage.
	Images and suchlike. You can choose where they will end up.</li>

	<li>When the imported site is given new URLs (reflecting the original
	path) we can publish the new nodes in a 'subsite' by applying a prefix
	directory to the aliases they are issued. The existing old links will
	be written to point to where the imported neighbouring pages are
	EXPECTED to end up. Incremental processing means they may not always be
	there until the whole site is done. Link checking (preferably
	on-the-fly) would be a nice tidy-up process.</li>

	<li>The new URLs generated for the new pages are url_aliases based on
	the original paths. You can choose to have tidy (no suffix) or legacy
	(old .htm or whatever suffix) aliases - or both.</li>

	<li>As the input is (a fragment of) Pure HTML, the content filter
	(input format) must be set correctly. I choose to define a blank
	filter, which doesn't even add extra BRs, but you can override that if
	you wish. <span class='beta'>Should this option be hidden, or is it
	useful?</span></li>

	<li>If using non-native HTMLTidy support, the path to the tidy
	executable should be defined. <span class='beta'>Security issue giving
	commandline access to a bin? Should move into server-side settings?</span></li>
</ul>

<h4><a name='Treeview' id="Treeview">Notes on the Treeview Interface</a></h4>

<p>Files and folders beginning with _ or . are nominally 'hidden' so are
skipped and do not show up on this listing. While it's possible to list
a thousand or so files, It may be a good idea to allow the listing to be
more selective, to scale to larger sites. Do this by entering the <strong>Subsection
to list</strong> <em>before</em> clicking list and waiting for every
file on the server to be enumerated.</p>

<h2><a name='Development' id="Development">Development / TODO</a></h2>

<p>As mentioned in Usage, this module uses no database tables of its
own. Pages are read straight into 'page' nodes. I guess it could feed
into flexinode if your import files had extra parsable content blocks,
and I've sucessfully used it to import other random XML formats
(RecipeML) although the advantages of doing so are limited.</p>

<p>It's easy to imagine this sytem set up as a synchroniser, that could
re-fetch and refresh local nodes when remote content changes. This would
involve recording exactly what the source URL was (which isn't currently
done) but would be a fun feature.</p>

<p>I may fork off the page-parsing into a pluggable method, so that a
regexp version can be developed alongside, and be used for folk without
XSL support.</p>

<p>How to leverage this to import a local site to a remote server? You
must either unpack the source files somewhere on that machine, then
provide the absolute path where the server can find them, or <span
	class='todo'>upload a zip package and I'll try to take it from
there.(TODO)</span></p>

<p class='todo'>Also TODO is a 'Spidering' method to try to import URL
sites. Way in the future!</p>

<p class='todo'><strike>TODO Allow settings to set import content type
to something other than 'page'</strike> done</p>

<p class='todo'>TODO Find a way to map more meta-data from the original
page (assuming there is any to be extracted) to Drupal properties, eg
get the contents of META keywords into Taxonomy associations</p>

<p class='todo'>TODO There are issure when a page links directly to a
file that would be regarded as a resource via an href. Most hrefs are
re-written to point to the new node, but things like large images or
word docs get imported under 'files'. The XSL rewrite_href_and_src.xsl
attempts to correct for this, but there may be some side-effects. Always
run a link checker after import.</p>

<h3 id='trouble'>Trouble</h3>

<p>The PHP4 XML parser (Sablotron) has trouble with duplicate attributes
- if found in a tag (like from old bad HTML) all subsequent input will
be flattened to plaintext. Older versions of HTMLTidy, however, do not
detect and fix these for us. Make sure that tidy supports <a
	href="http://tidy.sourceforge.net/docs/quickref.html#repeated-attributes">option
repeated-attributes</a>. It seems the commandline version fixed this
somewhere between the 2000 and 2004 release. (Not sure about the PHP
module version - it's PHP5, so should be OK)</p>
<p><a href="http://drupal.org/node/97532">An issue has been found</a>
running under</p>
<pre>PHP 4.3.11
xsltproc 1.0.16
libxml 2.6.2
libxslt 1.1.0
</pre>
<p>(possibly other similar configurations) whereby &amp;lt; and &amp;gt;
encoded entities in the input are prematurely converted to the &lt; and
&gt; s outputting unencoded results. We are pretty sure this is a
limitation of the old, incomplete XML implimentations of the time. An
upgrade to PHP 4.4 solved it in one case, so good luck.</p>
<p id="open_basedir">If your server has <a
	href="http://nz2.php.net/manual/en/features.safe-mode.php#ini.open-basedir">PHP
open_basedir</a> restrictions in effect, the webserver/PHP process may
be prevented from accessing files outside of webroot. This is a good
security measure, but may stop import_html from reading your source data
(even though browsing the source directories may still appear to work).
The <code>open_basedir</code> setting can be seen in your phpinfo. <br />
An error like: <code>Local file copy failed (/tmp/1fixed/simple.htm to
files/imported/simple.htm)</code> When you are sure the source file <strong>does</strong>
exist and permissions <strong>are</strong> readable may be symptomatic
of good security on your server. A reasonable fix is to place your
source data inside webroot/files (even if just temporarily) to run the
import process, then delete it later. Alternatively, copy your data over
top of web root (as described in walkthrough.htm) to do an in-place
import. Disabling open_basedir is not recommended, and probably requires
root privileges anyway. <a href="http://drupal.org/node/103221">Drupal.org
issue discussion</a></p>

<p>I've gone to great lengths to rewrite the links from the new node
locations to <em>relative</em> links to the resources that moved over
into /files/ but there are problems. When <code>a/long/path/index.html</code>
links to its image by going <code>../../../files/a/long/path/pic.jpg</code>
it <b>works</b> which is good. But as <code>a/long/path/index.html</code>
is also aliased to <code>a/long/path</code> - that up-and-over path is <em>wrong</em>
now the page is being served from what looks to the browser like a
different place.</p>
<p>I don't favour embedding anything that hard-codes the Drupal
base_url, and we don't want to use HTML BASE. I want to continue to
support portable subsites, so embedding site-rooted links (<code>/files/etc</code>)
is not great either.</p>
<p>Currently, by happy chance, going up one <code>../</code> too far
will get <em>ignored</em> by most browsers, so if you are <em>not</em>
running Drupal in a subdirectory, the requests for both style of page <em>will</em>
just work. Which will mean that 80% of cases should get by OK. The rest
may need an output filter of some sort developed some day</p>

<h3><a name='reference'>Reference</a></h3>

<p>Long ago, I started building this with reference to the existing <a
	href='http://drupal.org/node/14858'>import/export module</a> but I
couldn't find too many common features. The transitional format the XSL
templates convert into is a 'microformat' of XHTML (basically XHTML, but
with strictly controlled classes and IDs). This is how I see a
platform-agnostic dump of content should be exported, when this
eventually morphs into import_export_HTML.</p>

</body>
</html>
